#!/usr/bin/env python3
"""
Inferç«¯æµ‹è¯•è„šæœ¬
åŠŸèƒ½ï¼šåˆå§‹åŒ–TensorTableï¼Œåˆ›å»º100ä¸ª4000*500çš„torch.Tensorï¼Œè°ƒç”¨multi_getæ¥æ”¶æ•°æ®
æ”¯æŒå¾ªç¯æµ‹è¯•ä»¥éªŒè¯ç¨³å®šæ€§å’Œæ€§èƒ½
æ·»åŠ MD5æ ¡éªŒåŠŸèƒ½éªŒè¯æ•°æ®ä¼ è¾“æ­£ç¡®æ€§
"""

import torch
import sys
import os
import time
import logging
import argparse
import hashlib
from astate.parallel_config import ParallelConfig

# æ·»åŠ astateå®¢æˆ·ç«¯åº“åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.getcwd(), 'build', 'python'))

try:
    import astate
    from astate import ShardedKey, TensorTableType, TensorStorage
    print("âœ… Successfully imported astate")
except ImportError as e:
    print(f"âŒ Failed to import astate: {e}")
    print("è¯·ç¡®ä¿å·²ç¼–è¯‘Pythonç»‘å®šæ¨¡å—")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='[INFER] %(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


from typing import Any

def print_refcount(obj: Any, name: str = "Object") -> None:
    """
    Print the reference count of the given object.
    
    Args:
    obj (Any): The object whose reference count you want to check.
    name (str): A name to identify the object in the output (default: "Object").
    """
    count = sys.getrefcount(obj) - 1  # Subtract 1 to account for the temporary reference
    print(f"Reference count for {name}: {count}")

def calculate_tensor_md5(tensor):
    """è®¡ç®—tensorçš„MD5å€¼"""
    # å°†tensorè½¬æ¢ä¸ºè¿ç»­çš„å­—èŠ‚æµ
    tensor_bytes = tensor.detach().cpu().numpy().tobytes()
    return hashlib.md5(tensor_bytes).hexdigest()


def write_md5_to_file(tensor_data, iteration, seq_id, output_dir="md5_output"):
    """å°†tensorçš„MD5å€¼å†™å…¥æ–‡ä»¶ - å…ˆé‡æ„åˆ†ç‰‡tensorä¸ºå®Œæ•´tensorå†è®¡ç®—MD5"""
    os.makedirs(output_dir, exist_ok=True)

    md5_filename = os.path.join(
        output_dir, f"infer_md5_iter{iteration}_seq{seq_id}.txt")

    logger.info(f"è®¡ç®—å¹¶å†™å…¥MD5å€¼åˆ°æ–‡ä»¶: {md5_filename}")

    # ç¬¬ä¸€æ­¥ï¼šæŒ‰ç…§tensor_dataåˆ—è¡¨ä¸­key.keyå¯¹æ‰€æœ‰tensorè¿›è¡Œåˆ†ç»„
    tensor_groups = {}
    for key, tensor in tensor_data:
        tensor_key = key.key
        if tensor_key not in tensor_groups:
            tensor_groups[tensor_key] = []
        tensor_groups[tensor_key].append((key, tensor))
    
    logger.info(f"åˆ†ç»„ç»“æœ: å…±{len(tensor_groups)}ä¸ªtensorç»„")

    with open(md5_filename, 'w') as f:
        f.write(
            f"# Infer MD5 values - Iteration {iteration}, Seq ID {seq_id}\n")
        f.write(f"# Format: tensor_key,md5_hash,shape,dtype\n")

        # ç¬¬äºŒæ­¥ï¼šæŒ‰ç…§key.global_shapeåˆ›å»ºå®Œæ•´çš„tensorï¼ŒæŒ‰ç…§key.global_offsetå°†tensorçš„æ•°æ®æ‹·è´åˆ°å®Œæ•´tensorä¸­
        for tensor_key, key_tensor_list in tensor_groups.items():
            logger.info(f"å¤„ç†tensorç»„: {tensor_key} (åŒ…å«{len(key_tensor_list)}ä¸ªåˆ†ç‰‡)")
            
            # ä»ç¬¬ä¸€ä¸ªåˆ†ç‰‡è·å–å…¨å±€shapeå’Œdtypeä¿¡æ¯
            first_key, first_tensor = key_tensor_list[0]
            global_shape = first_key.globalShape
            dtype = first_tensor.dtype
            device = first_tensor.device
            
            logger.info(f"åˆ›å»ºå®Œæ•´tensor: shape={global_shape}, dtype={dtype}, device={device}")
            
            # åˆ›å»ºå®Œæ•´çš„tensorï¼ˆé›¶åˆå§‹åŒ–ï¼‰
            full_tensor = torch.zeros(global_shape, dtype=dtype, device=device)
            
            # å°†æ¯ä¸ªåˆ†ç‰‡çš„æ•°æ®æ‹·è´åˆ°å®Œæ•´tensorçš„å¯¹åº”ä½ç½®
            for key, tensor in key_tensor_list:
                global_offset = key.globalOffset
                tensor_shape = tensor.shape
                
                # æ„å»ºåˆ‡ç‰‡ç´¢å¼•
                slice_indices = []
                for i, (offset, size) in enumerate(zip(global_offset, tensor_shape)):
                    slice_indices.append(slice(offset, offset + size))
                
                # æ‹·è´åˆ†ç‰‡æ•°æ®åˆ°å®Œæ•´tensor
                full_tensor[tuple(slice_indices)] = tensor.clone()
                logger.debug(f"æ‹·è´åˆ†ç‰‡ {key.key} ä»offset {global_offset} åˆ°å®Œæ•´tensor")
            
            # è®¡ç®—å®Œæ•´tensorçš„MD5å€¼
            md5_hash = calculate_tensor_md5(full_tensor)
            shape_str = "x".join(map(str, global_shape))
            dtype_str = str(dtype)

            f.write(f"{tensor_key},{md5_hash},{shape_str},{dtype_str}\n")
            logger.info(f"âœ… {tensor_key}: MD5={md5_hash[:8]}..., shape={shape_str}")

    logger.info(f"âœ… MD5å€¼å·²å†™å…¥æ–‡ä»¶: {md5_filename}")
    return md5_filename


def zero_tensors(tensor_data):
    """å°†æ‰€æœ‰tensorçš„å…ƒç´ ç½®ä¸ºé›¶"""
    logger.info("å°†æ‰€æœ‰tensorå…ƒç´ ç½®ä¸ºé›¶...")
    for key, tensor in tensor_data:
        tensor.zero_()
    logger.info("âœ… æ‰€æœ‰tensorå·²ç½®é›¶")


def create_empty_tensors(num_tensors=100, height=4000, width=500, device='cpu', shard_rows=1, shard_cols=1):
    """åˆ›å»ºç”¨äºæ¥æ”¶æ•°æ®çš„ç©ºtensorï¼Œæ”¯æŒåˆ†ç‰‡è¯»å–"""
    logger.info(
        f"Creating {num_tensors} empty tensors of size {height}x{width} on {device}")
    logger.info(f"åˆ†ç‰‡é…ç½®: {shard_rows}è¡Œ x {shard_cols}åˆ— = {shard_rows * shard_cols}ä¸ªåˆ†ç‰‡")

    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if device == 'cuda' and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        device = 'cpu'
    
    # è®¾ç½®è®¾å¤‡
    torch_device = torch.device(device)
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {torch_device}")

    # è®¡ç®—åˆ†ç‰‡å°ºå¯¸
    shard_height = height // shard_rows
    shard_width = width // shard_cols
    
    # æ£€æŸ¥æ˜¯å¦èƒ½æ•´é™¤
    if height % shard_rows != 0:
        logger.warning(f"é«˜åº¦ {height} ä¸èƒ½è¢« {shard_rows} æ•´é™¤ï¼Œå°†ä½¿ç”¨å‘ä¸‹å–æ•´")
    if width % shard_cols != 0:
        logger.warning(f"å®½åº¦ {width} ä¸èƒ½è¢« {shard_cols} æ•´é™¤ï¼Œå°†ä½¿ç”¨å‘ä¸‹å–æ•´")
    
    logger.info(f"æ¯ä¸ªåˆ†ç‰‡å°ºå¯¸: {shard_height}x{shard_width}")

    tensor_data = []
    
    for tensor_idx in range(num_tensors):
        # ä¸ºæ¯ä¸ªåŸå§‹tensoråˆ›å»ºåˆ†ç‰‡è¯»å–
        for row in range(shard_rows):
            for col in range(shard_cols):
                # è®¡ç®—åˆ†ç‰‡çš„å®é™…å°ºå¯¸ï¼ˆå¤„ç†ä¸èƒ½æ•´é™¤çš„æƒ…å†µï¼‰
                # æœ€åä¸€è¡Œ/åˆ—çš„åˆ†ç‰‡éœ€è¦åŒ…å«æ‰€æœ‰å‰©ä½™çš„å…ƒç´ 
                actual_height = shard_height if row < shard_rows - 1 else height - row * shard_height
                actual_width = shard_width if col < shard_cols - 1 else width - col * shard_width
                
                # åˆ›å»ºåˆ†ç‰‡tensor
                tensor = torch.zeros(actual_height, actual_width, dtype=torch.float32, device=torch_device)

                # åˆ›å»ºShardedKeyï¼Œä½¿ç”¨åŸå§‹tensorçš„keyï¼Œä½†è®¾ç½®ä¸åŒçš„offsetå’Œshape
                key = ShardedKey()
                key.key = f"tensor_{tensor_idx:03d}"  # ä½¿ç”¨åŸå§‹tensorçš„key
                key.globalShape = [height, width]  # åŸå§‹tensorçš„å…¨å±€shape
                key.globalOffset = [row * shard_height, col * shard_width]  # åˆ†ç‰‡åœ¨å…¨å±€tensorä¸­çš„åç§»é‡

                tensor_data.append((key, tensor))

        if (tensor_idx + 1) % 10 == 0:
            logger.info(f"Created shard tensors for {tensor_idx + 1} original tensors...")

    logger.info(f"âœ… Created {len(tensor_data)} shard tensors successfully on {device}")
    return num_tensors, tensor_data


def verify_data(tensor_data):
    """éªŒè¯æ¥æ”¶åˆ°çš„æ•°æ®"""
    logger.info("Verifying received data...")

    non_zero_count = 0
    total_elements = 0

    for key, tensor in tensor_data:
        # è®¡ç®—éé›¶å…ƒç´ æ•°é‡
        non_zero = torch.count_nonzero(tensor).item()
        total = tensor.numel()

        non_zero_count += non_zero
        total_elements += total

        if non_zero > 0:
            logger.info(f"Tensor {key.key}: {non_zero}/{total} non-zero elements "
                        f"(mean: {tensor.mean().item():.4f}, std: {tensor.std().item():.4f})")

    percentage = (non_zero_count / total_elements) * \
        100 if total_elements > 0 else 0
    logger.info(
        f"Overall: {non_zero_count}/{total_elements} non-zero elements ({percentage:.2f}%)")

    return non_zero_count > 0


def parse_args():
    parser = argparse.ArgumentParser(description="Inferç«¯æµ‹è¯•è„šæœ¬ - æ”¯æŒå¾ªç¯æµ‹è¯•å’ŒMD5æ ¡éªŒ")
    parser.add_argument("--role_rank", type=int, default=0, help="role rank")
    parser.add_argument("--role_size", type=int, default=1, help="role size")
    parser.add_argument("--iterations", type=int,
                        default=5, help="å¾ªç¯æµ‹è¯•æ¬¡æ•° (é»˜è®¤: 5)")
    parser.add_argument("--sleep", type=int, default=3,
                        help="æ¯æ¬¡å¾ªç¯é—´éš”ç§’æ•° (é»˜è®¤: 3)")
    parser.add_argument("--num_tensors", type=int,
                        default=20, help="tensoræ•°é‡ (é»˜è®¤: 20)")
    parser.add_argument("--height", type=int, default=20000,
                        help="tensoré«˜åº¦ (é»˜è®¤: 20000)")
    parser.add_argument("--width", type=int, default=5000,
                        help="tensorå®½åº¦ (é»˜è®¤: 5000)")
    parser.add_argument("--seq_id", type=int, default=1, help="åºåˆ—ID (é»˜è®¤: 1)")
    parser.add_argument("--output_dir", type=str, default="md5_output",
                        help="MD5è¾“å‡ºç›®å½• (é»˜è®¤: md5_output)")
    parser.add_argument('--md5-check', action='store_true',
                        default=os.getenv(
                            'ASTATE_ENABLE_MD5_CHECK', '1') == '1',
                        help='æ˜¯å¦å¯ç”¨MD5æ ¡éªŒï¼Œé»˜è®¤å¯ç”¨')
    parser.add_argument('--random-sleep-max', type=float, default=float(os.getenv('ASTATE_RANDOM_SLEEP_MAX', 5)),
                        help='æ¯è½®æ¨ç†åæœ€å¤§éšæœºsleepç§’æ•°ï¼Œå¤§äº0å¯ç”¨éšæœºsleepï¼Œé»˜è®¤5')
    parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'],
                        help='tensoråˆ›å»ºè®¾å¤‡ (é»˜è®¤: cpu, å¯é€‰: cuda)')
    # æ–°å¢åˆ†ç‰‡å‚æ•°
    parser.add_argument('--shard_rows', type=int, default=1,
                        help='åˆ†ç‰‡è¡Œæ•° (é»˜è®¤: 1, ä¸åˆ†ç‰‡)')
    parser.add_argument('--shard_cols', type=int, default=1,
                        help='åˆ†ç‰‡åˆ—æ•° (é»˜è®¤: 1, ä¸åˆ†ç‰‡)')
    return parser.parse_args()


def main():
    args = parse_args()
    enable_md5 = args.md5_check
    random_sleep_max = args.random_sleep_max
    logger.info("ğŸš€ Starting Infer script...")

    # åˆ›å»ºREMOTEç±»å‹çš„TensorTable
    logger.info("Creating REMOTE TensorTable...")
    parallel_config = ParallelConfig.create_inference_config(
        role_size=args.role_size, role_rank=args.role_rank)
    table = astate.create_remote_table(
        "remote_tensor_table", parallel_config)
    logger.info("âœ… TensorTable created")

    # é…ç½®å¾ªç¯å‚æ•°
    num_iterations = args.iterations
    sleep_between_iterations = args.sleep
    seq_id = args.seq_id

    # è®°å½•æ€»ä½“ç»Ÿè®¡
    total_times = []
    successful_iterations = 0
    complete_times = []
    md5_files = []  # è®°å½•ç”Ÿæˆçš„MD5æ–‡ä»¶

    # åœ¨å¾ªç¯å¤–åˆ›å»ºtensorsï¼Œæ‰€æœ‰è¿­ä»£ä½¿ç”¨åŒä¸€æ‰¹tensor
    logger.info(
        f"ğŸ“¦ åˆ›å»ºå…±äº«çš„tensoræ•°æ® (tensors={args.num_tensors}, å°ºå¯¸={args.height}x{args.width}, è®¾å¤‡={args.device})...")
    logger.info(f"åˆ†ç‰‡é…ç½®: {args.shard_rows}è¡Œ x {args.shard_cols}åˆ—")
    original_num_tensors, tensor_data = create_empty_tensors(
        num_tensors=args.num_tensors, height=args.height, width=args.width, 
        device=args.device, shard_rows=args.shard_rows, shard_cols=args.shard_cols)

    logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {num_iterations} æ¬¡ multi_get å¾ªç¯æµ‹è¯•...")
    logger.info(
        f"ğŸ“‹ æµ‹è¯•å‚æ•°: tensors={args.num_tensors}, å°ºå¯¸={args.height}x{args.width}, seq_id={seq_id}")
    logger.info(f"ğŸ’¡ æ³¨æ„: æ‰€æœ‰è¿­ä»£ä½¿ç”¨åŒä¸€æ‰¹tensorå¯¹è±¡")

    for iteration in range(0, num_iterations):
        logger.info(f"\nğŸ”„ == ç¬¬ {iteration}/{num_iterations} æ¬¡è¿­ä»£ ===")

        try:
            # åœ¨æ¥æ”¶æ•°æ®å‰ï¼Œå°†æ‰€æœ‰tensorç½®é›¶
            logger.info(f"[è¿­ä»£{iteration}] å°†tensorç½®é›¶...")
            zero_tensors(tensor_data)

            # æ‰§è¡Œmulti_get
            logger.info(
                f"[è¿­ä»£{iteration}] è°ƒç”¨ multi_get (seq_id={seq_id})...")

            start_time = time.time()
            # for key, tensor in tensor_data:
            #     print_refcount(tensor, key.key)
            success = table.multi_get(seq_id + iteration, tensor_data)
            # for key, tensor in tensor_data:
            #     print_refcount(tensor, key.key)
            end_time = time.time()

            # è®¡ç®—å¹¶ä¿å­˜æ¥æ”¶åçš„MD5å€¼
            if enable_md5:
                logger.info(f"[è¿­ä»£{iteration}] è®¡ç®—æ¥æ”¶åçš„tensor MD5å€¼...")
                md5_file = write_md5_to_file(
                    tensor_data, iteration, seq_id + iteration, args.output_dir)
                md5_files.append(md5_file)

            logger.info(
                f"[è¿­ä»£{iteration}] è°ƒç”¨ complete (seq_id={seq_id + iteration})...")
            table.complete(seq_id + iteration)
            complete_time = time.time()

            iteration_time = end_time - start_time
            total_times.append(iteration_time)
            complete_time = complete_time - end_time
            complete_times.append(complete_time)

            if success:
                successful_iterations += 1
                logger.info(
                    f"[è¿­ä»£{iteration}] âœ… multi_get æˆåŠŸå®Œæˆï¼Œè€—æ—¶ {iteration_time:.2f} ç§’")
                logger.info(
                    f"[è¿­ä»£{iteration}] å¹³å‡æ¯ä¸ªtensoræ—¶é—´: {iteration_time/len(tensor_data)*1000:.2f} ms")
                logger.info(
                    f"[è¿­ä»£{iteration}] è°ƒç”¨ complete è€—æ—¶: {complete_time:.2f} ç§’")

                # éªŒè¯æ¥æ”¶åˆ°çš„æ•°æ®
                if verify_data(tensor_data):
                    logger.info(f"[è¿­ä»£{iteration}] âœ… æ•°æ®éªŒè¯æˆåŠŸ - æ¥æ”¶åˆ°éé›¶æ•°æ®")
                else:
                    logger.warning(
                        f"[è¿­ä»£{iteration}] âš ï¸ æ•°æ®éªŒè¯è­¦å‘Š - æ‰€æœ‰tensorséƒ½ä¸ºé›¶")

            else:
                logger.error(f"[è¿­ä»£{iteration}] âŒ multi_get å¤±è´¥")

            # éšæœºsleep
            if random_sleep_max > 0 and iteration != num_iterations - 1:
                import random
                sleep_time = random.uniform(0, random_sleep_max)
                logger.info(
                    f"[è¿­ä»£{iteration}] æœ¬è½®æ¨ç†ç»“æŸï¼Œéšæœºsleep {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)
            # åœ¨è¿­ä»£ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€æ¬¡ï¼‰
            elif iteration < num_iterations:
                logger.info(
                    f"[è¿­ä»£{iteration}] ç­‰å¾… {sleep_between_iterations} ç§’åç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£...")
                time.sleep(sleep_between_iterations)

        except Exception as e:
            logger.error(f"[è¿­ä»£{iteration}] âŒ è¿­ä»£è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    logger.info(f"Calling complete with seq_id={seq_id}...")
    table.complete(seq_id)

    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    logger.info(f"\nğŸ‰ === å¾ªç¯æµ‹è¯•ç»Ÿè®¡ç»“æœ ===")
    logger.info(f"æ€»è¿­ä»£æ¬¡æ•°: {num_iterations}")
    logger.info(f"æˆåŠŸæ¬¡æ•°: {successful_iterations}")
    logger.info(f"æˆåŠŸç‡: {successful_iterations/num_iterations*100:.1f}%")

    if total_times:
        logger.info(f"å¹³å‡è€—æ—¶: {sum(total_times)/len(total_times):.2f} ç§’")
        logger.info(f"æœ€å¿«è€—æ—¶: {min(total_times):.2f} ç§’")
        logger.info(f"æœ€æ…¢è€—æ—¶: {max(total_times):.2f} ç§’")
        logger.info(f"æ€»è€—æ—¶: {sum(total_times):.2f} ç§’")

        # è®¡ç®—ååé‡ç»Ÿè®¡
        avg_time = sum(total_times) / len(total_times)
        tensors_per_sec = original_num_tensors / avg_time
        mb_per_sec = (original_num_tensors * args.height *
                      # å‡è®¾float32
                      args.width * 4) / (1024 * 1024) / avg_time

        logger.info(f"å¹³å‡ååé‡: {tensors_per_sec:.1f} tensors/ç§’")
        logger.info(f"å¹³å‡æ•°æ®ååé‡: {mb_per_sec:.1f} MB/ç§’")
        logger.info(f"åˆ†ç‰‡åå®é™…tensoræ•°é‡: {len(tensor_data)} (åŸå§‹: {original_num_tensors})")

    if complete_times:
        logger.info(
            f"å¹³å‡completeè€—æ—¶: {sum(complete_times)/len(complete_times):.2f} ç§’")
        logger.info(f"æœ€å¿«completeè€—æ—¶: {min(complete_times):.2f} ç§’")
        logger.info(f"æœ€æ…¢completeè€—æ—¶: {max(complete_times):.2f} ç§’")
        logger.info(f"æ€»completeè€—æ—¶: {sum(complete_times):.2f} ç§’")

    # è¾“å‡ºMD5æ–‡ä»¶ä¿¡æ¯
    logger.info(f"\nğŸ“ === MD5æ–‡ä»¶ä¿¡æ¯ ===")
    logger.info(f"ç”Ÿæˆçš„MD5æ–‡ä»¶æ•°é‡: {len(md5_files)}")
    for md5_file in md5_files:
        logger.info(f"MD5æ–‡ä»¶: {md5_file}")

    if successful_iterations == num_iterations:
        logger.info("ğŸ‰ æ‰€æœ‰è¿­ä»£éƒ½æˆåŠŸå®Œæˆï¼Infer script æµ‹è¯•é€šè¿‡ï¼")
        return 0
    elif successful_iterations > 0:
        logger.warning(
            f"âš ï¸ éƒ¨åˆ†è¿­ä»£æˆåŠŸï¼Œ{num_iterations - successful_iterations} æ¬¡å¤±è´¥")
        return 1
    else:
        logger.error("âŒ æ‰€æœ‰è¿­ä»£éƒ½å¤±è´¥äº†")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
